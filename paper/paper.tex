% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\newdef{definition}{Definition}

\makeatletter
\newif\if@restonecol
\makeatother
\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumerate}


\begin{document}

\title{EasyMerge - A New Tool for Code Clones Refactoring}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Shengying Pan\\
       \affaddr{School of Computer Science}\\
       \affaddr{University of Waterloo}\\
       \email{s5pan@uwaterloo.ca}
% 2nd. author
\alignauthor
Haocheng Qin\\
       \affaddr{School of Computer Science}\\
       \affaddr{University of Waterloo}\\
       \email{h7qin@uwaterloo.ca}
% 3rd. author
\alignauthor Yahui Chen\\
       \affaddr{School of Computer Science}\\
       \affaddr{University of Waterloo}\\
       \email{y556chen@uwaterloo.ca}
\and  % use '\and' if you need 'another row' of author names
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Code clones are common in medium to large scale software projects. Oftentimes, unnecessary clones cause troubles to code base maintenance and code reusability. 
Over past decades, many techniques and approaches have been proposed to detect code clones. However, how to refactor clones is still a very challenging topic to software
engineers. Even text-wise identical code clones can be semantically different when they are referring variables and calling functions outside. And the problem is more 
complex when scopes and dependencies are involved. Furthermore, not all clones shall be refactored as they may be part of independent
libraries. And refactoring is not only about removing duplicate code but also fixing all reference errors caused by such deletion thereafter. 
As a result, we need adaptive clone refactoring tools that can locate unnecessary clones and alert possible implications in the procedure to help
software engineers be more efficient and make less mistakes in the refactoring process.

In this paper, we introduce EasyMerge, a new tool to refactor code clones. EasyMerge is built on top of AST-based anti-unification clone detection algorithm and 
refactors software projects written in Python. It adds intelligence to code clone refactoring by categorizing clones and generating refactoring recommendations
based on evaluation of external variable references and function calls. It copes with features of Python language and can deal with clone fragments across different scopes. 
It ensures functionality-wise consistency before and after refactoring and provide warnings when potential refactoring could lead to
unwanted complexity or cause readability issues.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.7}{Software Engineering}{Maintenance}[Restructuring, reverse engineering, and reengineering]

\terms{Algorithms}

\keywords{Software engineering, clone detection, code refactoring, recommendation system, Python} % NOT required for Proceedings

\section{Introduction}
In software development, it's very common seeing developers reuse code fragments by copying and pasting with or without minor adaptation.
Moreover, for large scale projects, developers are often too lazy to browse existing source files so that they may rewrite similar or even identical functions which
were already in the code base. As a result, software systems often contain sections of code that are very similar, called code clones.

Previous research shows that a significant fraction (between 7\% and 23\%) of the code in a typical software system has been cloned \cite{baker} \cite{roy1}. Many code clones
in code bases are unnecessary duplications. 
Code duplication can be a significant drawback, leading to bad design, and increased probability of bug occurrence and propagation. As a result, it can significantly
increase maintenance cost, and form a barrier for software evolution. By detecting, categorizing,
and removing code clones, we can produce easier to understand, cleaner, and more reusable code.

Clone detection has been an avid research topic in the field of software engineering for decades. Fortunately, several automated techniques for detecting code clones
have already been proposed. However, how to deal with detected clones, e.g. how to distinguish necessary clones from unnecessary ones and how to refactor code to remove
unnecessary clones still remain a big problem in not only commercial but also academic domain. As a result, in this paper, we try to classify code clones and build
a recommendation system called EasyMerge to help developers merge unnecessary clones on top of current state-of-the-art clone detection approach.

More specifically, we pick CloneDigger \cite{bulychev}, an anti-unification duplicate code detection tool as our underlying clone detection approach. CloneDigger is one of the best available 
clone detection tools currently for its overall performance, coverage of multiple clone types, and availability. 
EasyMerge integrates CloneDigger as the pre-processing tool, analyze its output clone pairs, and recommend possible merges which can remove unnecessary clones without changing functionality of code base, creating reference conflicts, nor causing troubles to future code understanding and development.

The rest of the paper is structured as follows: we first go through the basics, background, and current state of clone detection and code clone refactoring in general.
Then we introduce and discuss the fundamentals of CloneDigger and the anti-unification algorithm it is using to detect clones. Afterwards, we explain EasyMerge's
work flow and underlying techniques. And at the end, we set up testing environment and discuss the experimental results of running EasyMerge against several
open source projects of different scales.

\section{Background}
\subsection{Clone Detection}
Roy, Cordy, and Koschke have done a great work \cite{roy2} writing an overview paper explaining the basics of clone detection, and providing a complete comparison of essential
strengths and weaknesses of both individual tools and techniques and alternative approaches in general. It gives us all the needed preliminaries to focus on clone refactoring rather
than spending time working on the detection part. We begin with a basic introduction to clone detection terminology in Roy's paper.

\begin{definition}
(Code Fragment). A code fragment (CF) is any sequence of code lines (with or without comments). It can be of any granularity, e.g., function
definition, begin-end block, or sequence of statements. A $CF$ is identified by its file name and begin-end line numbers in the original code base
and is denoted as a triple ($CF.FileName$, $CF.BeginLine$, $CF.EndLine$).
\end{definition}

\begin{definition}
(Code Clone). A code fragment $CF2$ is a clone of another code fragment $CF1$ if they are similar by some given definition of similarity, that is, 
$f(CF1) = f(CF2)$ where $f$ is the similarity function (see clone types below). Two fragments that are similar to each other form a clone pair
$(CF1, CF2)$, and when many fragments are similar, they form a clone class or clone group.
\end{definition}

\begin{definition}
(Clone Types). There are two main kinds of similarity between code fragments. Fragments can be similar based on the similarity of their program text,
or they can be similar based on their functionality (independent of their text). The first kind of clone is often the result of copying a code fragment and
pasting into another location. In the following we provide the types of clones based on both the textual (Type 1 to 3)\cite{bellon} and functional (Type 4)\cite{gabel, komondoor} similarities:

\begin{itemize}
\item {\bf Type-1:} Identical code fragments except for variations in whitespace, layout and comments.
\item {\bf Type-2:} Syntactically identical fragments except for variations in identifiers, literals, types, whitespace, layout and comments.
\item {\bf Type-3:} Copied fragments with further modifications such as changed, added or removed statements, in addition to variations in identifiers, literals, types, whitespace, layout and comments.
\item {\bf Type-4:} Two or more code fragments that perform the same computation but are implemented by different syntactic variants.
\end{itemize}
\end{definition}

There are many different techniques to detect code clones. In Roy's paper \cite{roy2}, they covered tools of textual approaches, lexical approaches, syntactic approaches,
semantic approaches and hybrids. Different tools have advantages and disadvantages for different facets (usage, interaction, language, clone information, technical aspect, 
adjustment, processing, and evaluation) and behave quite differently for some scenarios. As clone detection is still an on-going research, it's hard to say one tool is the best
at the moment. However, for the purpose of building EasyMerge, we want to pick a tool that is cross-platform, freely available, efficient, covering multiple clone types, and can
handle different editing scenarios decently. Thus we chose CloneDigger \cite{bulychev} after a comprehensive comparison.

\subsection{Clone Refactoring}
On the other hand, for clone refactoring, most proposed solutions use some pre-defined metrics as the refactoring guidance to help users determine whether the 
code clones are suitable for refactoring. For example, Balazinska \cite{balazinska} proposed to use 21 metrics to measure the suitability for refactoring. Higo \cite{higo}
developed ARIES tools, using the code detection tool CCFinder and environment analysis tool Gemini to find the clones can be refactored. And Schulze \cite{schulze}
proposed a new metric DIST (distance of the code clones). Unlike them, instead of calculating distance between code fragments and only merging close clones, EasyMerge 
try to fix the differences and merge as many clones as possible without introducing extra complexity.

For the actual merging process, currently existing research mainly focus on using refactoring patterns \cite{fowler}, especially ``Extract Method" and ``Pull Up Method".
"Extract Method" means that a fragment of source code is extracted and redefined as a new method. ``Pull Up Method" means that the same methods defined in child
classes are pulled up to its parent class. In general, they are both merging similar code blocks into one newly created function, and ``Pull Up Method" only differs from 
``Extract Method" as it places the newly created function into the super class when code blocks are inside functions sharing the same parent class.

Unfortunately, code clones may be coupled with surrounding code, its inheritance tree, or other independent classes/code blocks. For instance, code fragments inside clone pairs
may contain function calls or variable references which are not defined inside the fragments. Furthermore, functions and variables of the same names may be in fact different
at different locations of the code base. For example, if we have a code clone fragment as following:

\IncMargin{1em}
\begin{algorithm}
	for x in range(0, 3):
	
		\Indp a = x + 1
		
		b = f(x)
		
		c = d + x
\end{algorithm}
\DecMargin{1em}

We are calling function {\bf f()} where the definition of {\bf f()} can be different from fragment to fragment. Moreover, the value of variable {\bf d} is potentially different, too.
The current approach to solve this problem is to pass such external functions and variables as parameters to newly generated methods from the ``Extract Method" refactoring pattern.
For instance:

\IncMargin{1em}
\begin{algorithm}
	def helper(p1, p2):
	
		\Indp for x in range(0, 3):
		
			\Indp a = x + 1
			
			b = p1(x)
			
			c = p2 + x
			
		\Indm \Indm helper(f, d)
	
\end{algorithm}
\DecMargin{1em}

However, this creates two new problems. Firstly, in this example, the scope of b was changed. It is now inside the helper function and no longer available to code after the clone fragment
which could potentially cause reference errors. Secondly, when the length of the clone fragment is too long, we could possibly end up with a helper function of tons of parameters
which in turn make the code after refactoring harder to understand and is against our goal to improve readability. And the most intuitive approach to solve the first problem
is to return b and such variables from the ``Extracted Method" which introduces even more complexity.

As a result, for recently proposed refactoring approach \cite{li} and the tools mentioned at the beginning of this subsection, they count the number of such external references
in a code clone fragment as the key factor of their refactoring guideline metrics. 
And clone fragments of too many such occurrences will not be recommended for refactoring.
Furthermore, in Li's work \cite{li}, after standardization, it can also be used to refactor semantical
similar but syntax-wise different clones by moving different function references to extracted method's parameters.

Unlike most of current clone refactoring approaches that use token-based CCFinder \cite{kamiya} for clone detection, EasyMerge uses a newer detection tool called CloneDigger.
Compared to CCFinder, CloneDigger is cross-platform and freely available as an open source project. Moreover, based on evaluation from Roy \cite{roy2},
CloneDigger performs no worse than CCFinder in all detection scenarios and is actually better in certain scenarios e.g. a programmer copies a function that calculates the sum
and product of a loop variable and calls another function, foo() with these values as parameters three times, making changes in whitespace in the first fragment (S1(a)), changes
in commenting in the second (S1(b)), and changes in formatting in the third (S1(c)). In the following section, we will go over the basic ideas behind CloneDigger and its anti-unification
algorithm.

\section{CloneDigger}
\subsection{Overview}
Techniques for detecting duplicate code can be classified according to several criteria. Code can be viewed as similar based on syntactic criteria or at a semantic level. CloneDigger considers only syntactic similarity. The algorithm is approached based on abstract syntax trees. The algorithm of finding duplicates consists of several phases. In the beginning all statements are partitioned into clusters using anti-unification distance and the code is abstractly viewed as sequence of cluster identifiers. All pairs of identical sequences of cluster IDs, which have similar statements in corresponding positions, are globally checked for similarity using anti-unification distance.


\subsection{Preliminaries}
Anti-unification is a general expression of two given terms. Let $E_{1}$ and $E_{2}$ be two terms. Anti-unification E is a generalization of $E_{1}$ and $E_{2}$ if there exist two substitutions $\sigma_{1}$ and $\sigma_{2}$ such that $\sigma_{1}(E)=E_{1}$ and $\sigma_{2}(E)=E_{2}$. The most specific generalization of $E_{1}$ and $E_{2}$. The most specific generalization of $E_{1}$ and $E_{2}$ is called anti-unifier. The anti-unifier tree of two trees $T_{1}$ and $T_{2}$ is obtained by replacing some subtrees in $T_{1}$ and $T_{2}$ by special nodes.m containing term placeholders which are marked with integers, such as $?_{n}$. An anti-unifier stores only the common top-level tree structure and therefore details may be ignored.

The anti-unification distance is defined as follows: let U be the anti-unifier of two trees $T_{1}$ and $T_{2}$ with substitutions $\sigma_{1}$ and $\sigma_{2}$. $\sigma_{1}$ and $\sigma_{2}$ are mappings from the set \{$?_{1}$, $?_{2}$, ..., $?_{n}$\} to substituting trees. Anti-unification distance between $T_{1}$ and $T_{2}$ as a sum of sizes of substituting trees in $\sigma_{1}$ and $\sigma_{2}$. The distance doesn't allow the permutation of siblings or changing the number of child nodes.


\subsection{Duplicate Code Detection Algorithm}
The method of finding duplicate code consists of three phases:
\begin{enumerate}[step 1]

    \item Partitioning similar statements into clusters
    
    Identify similar statements using anti-unification and partition them into clusters. A two-pass clustering algorithm is used. The first pass of the algorithm compare each new statement with the anti-unifiers of all existing clusters. The function $add\_cost$ is used to compute the cost of adding a tree $T$ to the cluster consisting of $n$ trees with anti-unifier $au$. Let $au'$ be the result of anti-unification of $T$ and $au$ with substitutions $\sigma_{1}$ and $\sigma_{2}$: $\sigma_{1}(au') = au$, $\sigma_{2}(au') = T$. $add\_cost$ is defined as $n \times |\sigma_{1}| + |\sigma_{2}|$. During the second pass all the statements re traversed again and for each statement we search for the most similar pattern from the set produced in the previous pass using the anti-unification distance. After the first phase each statement is marked with its cluster ID, two statements with the same cluster ID are considered similar in this preliminary view.

    \item Finding pairs of identical cluster sequences
    All pairs of sequences of statements, which are large enough, are identically labeled.
    
    \item Examining code sequences for overall similarity
    All candidates are checked as a whole using anti-unification distance. The occurrences of the same variable refers to one leaf in the abstract syntax tree increase the quality of the algorithm.
\end{enumerate}

Now we can move on to introduce EasyMerge and how it works behind the scene.


\section{EasyMerge}

\section{Experimental Results}

%ACKNOWLEDGMENTS are optional
%\section{Acknowledgements}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A
%\subsection{References}
%Generated by bibtex from your ~.bib file.  Run latex,
%then bibtex, then latex twice (to resolve references)
%to create the ~.bbl file.  Insert that ~.bbl file into
%the .tex source file and comment out
%the command \texttt{{\char'134}thebibliography}.
%\balancecolumns
% That's all folks!
\end{document}
